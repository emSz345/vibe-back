// routes/chat.ts
import { Router, Request, Response } from 'express';
import { InferenceClient } from '@huggingface/inference';

// Importa√ß√£o dos servi√ßos necess√°rios
import IntentAnalysisService from '../services/IntentAnalysisService';
import ChatOrchestrator, { IResultadoChat } from '../services/ChatOrchestrator';
import SystemInfoService from '../services/SystemInfoService';
import ChatContext from '../models/ChatContext';

const router = Router();
// Inicializa o cliente de infer√™ncia do Hugging Face com o token da vari√°vel de ambiente
const client = new InferenceClient(process.env.HF_TOKEN as string);

/**
 * PROMPT DO SISTEMA - Define a personalidade e restri√ß√µes do bot
 * - Papel: Assistente virtual da plataforma NaVibe
 * - Restri√ß√µes: N√£o pode vender ingressos ou gerenciar carrinho
 * - Escopo: Informa√ß√µes gerais sobre a plataforma
 * - Formato: Texto puro com respostas amig√°veis
 */
const SYSTEM_PROMPT = `
Voc√™ √© o "Vibe Bot", um assistente virtual da plataforma NaVibe.
Seu papel √© ajudar usu√°rios com informa√ß√µes gerais sobre a plataforma.

üö´ **RESTRI√á√ïES:**
- N√ÉO fa√ßa vendas de ingressos
- N√ÉO gerencie carrinho de compras
- N√ÉO mostre eventos espec√≠ficos
- Foque em informa√ß√µes gerais e suporte

üí¨ **ESCOPO PERMITIDO:**
- Explicar sobre a plataforma
- Tirar d√∫vidas sobre funcionalidades
- Ajudar com cadastro e login
- Informa√ß√µes sobre categorias de eventos

FORMATO:
- Apenas texto puro com a resposta amig√°vel
- Use emojis quando apropriado
- Seja direto e natural
`;

// Inicializa√ß√£o dos servi√ßos
const intentAnalysisService = new IntentAnalysisService();
const systemInfoService = new SystemInfoService();
const chatOrchestrator = new ChatOrchestrator(
  intentAnalysisService,
  systemInfoService
);

// Interface para tipagem do corpo da requisi√ß√£o
interface ChatRequestBody {
  message: string;
}

/**
 * ROTA /chat - Endpoint principal para conversa√ß√£o
 * M√©todo: POST
 * 
 * Fluxo:
 * 1. Recebe mensagem do usu√°rio
 * 2. Valida dados de entrada
 * 3. Processa atrav√©s do ChatOrchestrator
 * 4. Gera resposta via IA
 * 5. Retorna resposta formatada
 */
router.post('/chat', async (req: Request, res: Response) => {
  try {
    // Extrai mensagem e user-id do cabe√ßalho
    const { message } = req.body as ChatRequestBody;
    const userId = (req.headers['user-id'] as string) || 'anonymous';

    // Valida√ß√£o b√°sica da mensagem
    if (!message?.trim()) {
      return res.status(400).json({ success: false, error: 'Mensagem √© obrigat√≥ria' });
    }

    // Cria contexto do usu√°rio para manter estado da conversa
    const userContext = new ChatContext(userId, {});
    
    // Processa a mensagem atrav√©s do orquestrador
    const processingResult: IResultadoChat = await chatOrchestrator.processarMensagem(
      message.trim(),
      userContext
    );

    // Gera resposta usando modelo de linguagem
    const respostaAI = await gerarRespostaAI(message);

    // Combina resultado do processamento com resposta da IA
    const respostaFinal: IResultadoChat = {
      ...processingResult,
      textoResposta: respostaAI
    };

    // Retorna resposta com sucesso
    res.json({
      success: true,
      reply: respostaFinal
    });

  } catch (error: any) {
    // Log detalhado do erro em ambiente de desenvolvimento
    console.error('‚ùå [CHAT] Erro:', error);

    // Resposta de fallback em caso de erro
    res.json({
      success: true,
      reply: {
        textoResposta: "E a√≠! üëã Tive um probleminha aqui, mas j√° estou me recuperando! Pode falar de novo? üòä",
        showCommands: true,
        state: {},
        quickReplies: [
          { text: "‚ùì Ajuda", action: "ajuda" },
          { text: "‚ÑπÔ∏è Sobre", action: "sobre" }
        ]
      }
    });
  }
});

/**
 * FUN√á√ÉO gerarRespostaAI - Gera resposta usando modelo de linguagem
 * @param mensagem - Texto enviado pelo usu√°rio
 * @returns Resposta gerada pela IA
 */
async function gerarRespostaAI(
  mensagem: string,
): Promise<string> {
  try {
    // Contexto formatado para o modelo de linguagem
    const contexto = `
MENSAGEM DO USU√ÅRIO: "${mensagem}"

CONTEXTO: Apenas conversa√ß√£o geral sobre a plataforma NaVibe.
`.trim();

    // Chamada para o modelo de linguagem
    const chatCompletion = await client.chatCompletion({
      model: "openai/gpt-oss-120b",
      messages: [
        { role: "system", content: SYSTEM_PROMPT },
        { role: "user", content: contexto }
      ],
      max_tokens: 800,
      temperature: 0.8
    }) as any;

    // Extrai resposta do completion
    const resposta = chatCompletion.choices[0].message.content || "";
    return resposta;

  } catch (error) {
    // Fallback em caso de erro na gera√ß√£o da resposta
    console.error('‚ùå [AI] Erro ao gerar resposta:', error);
    return "E a√≠! üëã Como posso te ajudar hoje com a plataforma NaVibe? üòä";
  }
}

export default router;